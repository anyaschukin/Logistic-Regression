{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique - Exercices tirés du MOOC d'Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données représente les chances de différents étudiants d'être admis à un programme universitaire en fonction de leurs résultats à deux examens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger les données du fichier ex2data1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ex2data1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette semaine nous vous fournissons la visualisation des données, mais normalement vous auriez à produire un graphe semblable avec matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Découpez vos données en une matrice X et un vecteur y et transformerz-les en array numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.values[:,:2] # converts dataframe to numpy array\n",
    "ones = np.ones([X.shape[0],1])\n",
    "X = np.concatenate((ones,X),axis=1)\n",
    "\n",
    "y = np.array(data.admission).reshape(100,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez bien les dimensions de vos structures de données (X.shape)  \n",
    "Rappelez-vous qu'il est judicieux de fixer les dimensions des vecteurs, par ex. (3,) avec la fonction reshape(3,1).\n",
    "La matrice X doit-elle être de dimensions m x n ou bien m x (n+1) ? Quelle est la valeur de n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisez theta en un vecteur de zéros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de zéros vous faudra-t-il....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(3).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation de l'hypothèse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revoyez l'équation de l'hypothèse de la régression logistique. Le produit de theta et de X est enveloppé dans une fonction g(z) qui correspond à la fonction sigmoïde. Nous allons commencer par coder cette fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Écrivez une fonction _sigmoid_ qui applique la sigmoïde à son argument et retourne le résultat. Si elle reçoit une matrice ou un vecteur en input, elle doit s'appliquer sur chaque élément individuellement et retourner une structure de mêmes dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez votre fonction. Quelle valeur renvoie une sigmoïde si z=0? Si z est grand? Si z est petit?  \n",
    "Il est possible que vous ayez un bug lorsque la fonction exponentielle reçoit des valeurs trop grandes. Dans ce cas vous pourrez éventuellement remplacer votre fonction sigmoïde par celle de scipy pour éviter des problèmes dans le reste de votre implémentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Écrivez une fonction _predict_, qui correspond à l'hypohèse hθ(x), qui prend en paramètres X et theta, applique l'hypothèse du modèle avec la fonction sigmoide, et se débrouille pour que le résultat final soit un vecteur de 1 et de 0 correspondant aux catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    z = sigmoid(np.dot(X, theta))\n",
    "    z[(z == 0)] += 10e-6    # takes care of bug\n",
    "    z[(z == 1)] -= 10e-6\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définissez la fonction de coût de votre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    hyp = predict(X, theta)\n",
    "    part1 = np.dot(-y.T, np.log(hyp))\n",
    "    part2 = np.dot((1 - y).T, np.log(1 - hyp))\n",
    "    return (part1 - part2) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculez le coût de votre modèle non entraîné. Vous devriez obtenir une valeur d'environ 0.693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59725911]\n",
      " [0.58159797]\n",
      " [0.59966471]\n",
      " [0.66096799]\n",
      " [0.7026212 ]\n",
      " [0.62056974]\n",
      " [0.6644169 ]\n",
      " [0.69030333]\n",
      " [0.69755796]\n",
      " [0.71071022]\n",
      " [0.73421517]\n",
      " [0.68836777]\n",
      " [0.70989999]\n",
      " [0.68365103]\n",
      " [0.60936787]\n",
      " [0.64654282]\n",
      " [0.67750872]\n",
      " [0.6741767 ]\n",
      " [0.6860178 ]\n",
      " [0.69480258]\n",
      " [0.67237379]\n",
      " [0.72441421]\n",
      " [0.63301025]\n",
      " [0.59165551]\n",
      " [0.69942947]\n",
      " [0.66383623]\n",
      " [0.70159763]\n",
      " [0.72860025]\n",
      " [0.66033363]\n",
      " [0.60600797]\n",
      " [0.66209644]\n",
      " [0.7143531 ]\n",
      " [0.63869111]\n",
      " [0.63935274]\n",
      " [0.6104686 ]\n",
      " [0.64340477]\n",
      " [0.59827089]\n",
      " [0.66965331]\n",
      " [0.68917695]\n",
      " [0.59576963]\n",
      " [0.71103172]\n",
      " [0.63522347]\n",
      " [0.73428783]\n",
      " [0.70588639]\n",
      " [0.63386953]\n",
      " [0.6614853 ]\n",
      " [0.69798517]\n",
      " [0.74332836]\n",
      " [0.66671222]\n",
      " [0.73088905]\n",
      " [0.70448491]\n",
      " [0.74359564]\n",
      " [0.72373879]\n",
      " [0.59463371]\n",
      " [0.6325265 ]\n",
      " [0.63211558]\n",
      " [0.74116297]\n",
      " [0.59444101]\n",
      " [0.69133932]\n",
      " [0.68684564]\n",
      " [0.69579825]\n",
      " [0.5947619 ]\n",
      " [0.64564704]\n",
      " [0.58179002]\n",
      " [0.62087612]\n",
      " [0.67027518]\n",
      " [0.61451994]\n",
      " [0.62982383]\n",
      " [0.70730082]\n",
      " [0.67316064]\n",
      " [0.58774224]\n",
      " [0.66897663]\n",
      " [0.6902187 ]\n",
      " [0.65996016]\n",
      " [0.65648064]\n",
      " [0.74591527]\n",
      " [0.63015973]\n",
      " [0.6363189 ]\n",
      " [0.65610499]\n",
      " [0.70582005]\n",
      " [0.72325742]\n",
      " [0.73291964]\n",
      " [0.67517169]\n",
      " [0.6505937 ]\n",
      " [0.70735687]\n",
      " [0.68013498]\n",
      " [0.61607485]\n",
      " [0.6965328 ]\n",
      " [0.70423484]\n",
      " [0.63896947]\n",
      " [0.73483577]\n",
      " [0.728441  ]\n",
      " [0.64331378]\n",
      " [0.69367498]\n",
      " [0.72248259]\n",
      " [0.70922398]\n",
      " [0.61762978]\n",
      " [0.74450907]\n",
      " [0.64673153]\n",
      " [0.69486363]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.62884748]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Écrivez une fonction _fit_ qui prend en arguments le vecteur X et le vecteur y des données d'entraînement et renvoie le vecteur de paramètres _theta_ qui a été appris, ainsi que l'évolution du coût"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noter que l'exercice original ne fait pas faire la descente du gradient pour entraîner le modèle, mais plutôt une fonction d'optimisation avancée (_fminunc_ en Matlab). Nous tenterons de faire quand même la descente du gradient. Les plus téméraires peuvent aussi trouver une fonction d'optimisation équivalente en Python et comparer les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, theta, alpha, num_iters):\n",
    "    m = X.shape[0]\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        hyp = predict(X, theta)\n",
    "        theta = theta - (alpha/m) * np.dot(X.T, (hyp - y))\n",
    "        J_history.append(float(cost(X, y, theta))) # float returns a float (if you do just cost() it appends an array to an array)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancez l'apprentissage en appelant la fonction _fit_ et en prenant bien soin de récupérer le résultat de *theta* à la fin!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyez entre vous quelles valeurs semblent correctes pour alpha et num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.000002\n",
    "num_iters = 100000\n",
    "theta = np.zeros(3).reshape(3,1)\n",
    "theta, J_history = fit(X, y, theta, alpha, num_iters)\n",
    "\n",
    "#print(theta, J_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appelez la fonction _cost_ avec le nouveau theta après entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devriez obtenir une valeur autour de 0.203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62884748]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On visualise maintenant l'évolution du coût en fonction du nombre d'itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6926062393451751,\n",
       " 0.6920701743172522,\n",
       " 0.6915389414380102,\n",
       " 0.6910124970409476,\n",
       " 0.690490797828935,\n",
       " 0.6899738008719168,\n",
       " 0.689461463604603,\n",
       " 0.6889537438241556,\n",
       " 0.6884505996878684,\n",
       " 0.687951989710841,\n",
       " 0.6874578727636492,\n",
       " 0.6869682080700094,\n",
       " 0.6864829552044409,\n",
       " 0.6860020740899243,\n",
       " 0.6855255249955576,\n",
       " 0.6850532685342106,\n",
       " 0.6845852656601771,\n",
       " 0.684121477666828,\n",
       " 0.6836618661842617,\n",
       " 0.6832063931769565,\n",
       " 0.6827550209414232,\n",
       " 0.6823077121038572,\n",
       " 0.6818644296177956,\n",
       " 0.6814251367617722,\n",
       " 0.6809897971369782,\n",
       " 0.6805583746649237,\n",
       " 0.6801308335851046,\n",
       " 0.6797071384526694,\n",
       " 0.6792872541360947,\n",
       " 0.6788711458148623,\n",
       " 0.6784587789771404,\n",
       " 0.6780501194174724,\n",
       " 0.6776451332344695,\n",
       " 0.6772437868285097,\n",
       " 0.6768460468994419,\n",
       " 0.6764518804442977,\n",
       " 0.6760612547550102,\n",
       " 0.675674137416137,\n",
       " 0.6752904963025945,\n",
       " 0.6749102995773965,\n",
       " 0.6745335156894019,\n",
       " 0.6741601133710712,\n",
       " 0.673790061636229,\n",
       " 0.6734233297778381,\n",
       " 0.6730598873657795,\n",
       " 0.6726997042446424,\n",
       " 0.6723427505315256,\n",
       " 0.6719889966138435,\n",
       " 0.671638413147146,\n",
       " 0.6712909710529464,\n",
       " 0.670946641516559,\n",
       " 0.6706053959849473,\n",
       " 0.670267206164582,\n",
       " 0.6699320440193105,\n",
       " 0.6695998817682355,\n",
       " 0.6692706918836042,\n",
       " 0.6689444470887116,\n",
       " 0.6686211203558095,\n",
       " 0.6683006849040308,\n",
       " 0.667983114197323,\n",
       " 0.6676683819423943,\n",
       " 0.6673564620866693,\n",
       " 0.6670473288162577,\n",
       " 0.6667409565539347,\n",
       " 0.666437319957132,\n",
       " 0.6661363939159409,\n",
       " 0.6658381535511283,\n",
       " 0.6655425742121628,\n",
       " 0.6652496314752554,\n",
       " 0.6649593011414084,\n",
       " 0.6646715592344811,\n",
       " 0.6643863819992631,\n",
       " 0.664103745899564,\n",
       " 0.6638236276163121,\n",
       " 0.6635460040456664,\n",
       " 0.663270852297142,\n",
       " 0.6629981496917471,\n",
       " 0.6627278737601315,\n",
       " 0.6624600022407496,\n",
       " 0.6621945130780333,\n",
       " 0.6619313844205805,\n",
       " 0.6616705946193525,\n",
       " 0.6614121222258867,\n",
       " 0.6611559459905207,\n",
       " 0.6609020448606295,\n",
       " 0.6606503979788737,\n",
       " 0.6604009846814624,\n",
       " 0.6601537844964267,\n",
       " 0.6599087771419068,\n",
       " 0.6596659425244519,\n",
       " 0.6594252607373307,\n",
       " 0.6591867120588568,\n",
       " 0.6589502769507257,\n",
       " 0.6587159360563619,\n",
       " 0.6584836701992833,\n",
       " 0.6582534603814735,\n",
       " 0.6580252877817682,\n",
       " 0.6577991337542548,\n",
       " 0.6575749798266824,\n",
       " 0.6573528076988859,\n",
       " 0.6571325992412212,\n",
       " 0.6569143364930135,\n",
       " 0.6566980016610162,\n",
       " 0.6564835771178843,\n",
       " 0.6562710454006576,\n",
       " 0.6560603892092578,\n",
       " 0.6558515914049959,\n",
       " 0.655644635009094,\n",
       " 0.6554395032012152,\n",
       " 0.6552361793180101,\n",
       " 0.6550346468516716,\n",
       " 0.654834889448502,\n",
       " 0.6546368909074937,\n",
       " 0.65444063517892,\n",
       " 0.6542461063629372,\n",
       " 0.6540532887082001,\n",
       " 0.6538621666104865,\n",
       " 0.6536727246113359,\n",
       " 0.6534849473966975,\n",
       " 0.6532988197955902,\n",
       " 0.6531143267787748,\n",
       " 0.6529314534574359,\n",
       " 0.6527501850818758,\n",
       " 0.6525705070402197,\n",
       " 0.6523924048571322,\n",
       " 0.652215864192543,\n",
       " 0.6520408708403856,\n",
       " 0.6518674107273464,\n",
       " 0.6516954699116241,\n",
       " 0.6515250345816989,\n",
       " 0.6513560910551157,\n",
       " 0.6511886257772739,\n",
       " 0.6510226253202296,\n",
       " 0.6508580763815105,\n",
       " 0.6506949657829358,\n",
       " 0.6505332804694518,\n",
       " 0.6503730075079747,\n",
       " 0.6502141340862453,\n",
       " 0.6500566475116915,\n",
       " 0.6499005352103047,\n",
       " 0.6497457847255214,\n",
       " 0.6495923837171199,\n",
       " 0.6494403199601223,\n",
       " 0.6492895813437087,\n",
       " 0.6491401558701417,\n",
       " 0.6489920316536987,\n",
       " 0.6488451969196146,\n",
       " 0.6486996400030347,\n",
       " 0.648555349347976,\n",
       " 0.6484123135062992,\n",
       " 0.6482705211366883,\n",
       " 0.6481299610036405,\n",
       " 0.6479906219764662,\n",
       " 0.6478524930282955,\n",
       " 0.6477155632350974,\n",
       " 0.6475798217747036,\n",
       " 0.6474452579258457,\n",
       " 0.6473118610671976,\n",
       " 0.6471796206764293,\n",
       " 0.6470485263292679,\n",
       " 0.646918567698567,\n",
       " 0.6467897345533867,\n",
       " 0.6466620167580801,\n",
       " 0.6465354042713883,\n",
       " 0.6464098871455454,\n",
       " 0.6462854555253905,\n",
       " 0.646162099647488,\n",
       " 0.6460398098392566,\n",
       " 0.6459185765181061,\n",
       " 0.6457983901905823,\n",
       " 0.6456792414515194,\n",
       " 0.6455611209832023,\n",
       " 0.6454440195545328,\n",
       " 0.6453279280202089,\n",
       " 0.6452128373199063,\n",
       " 0.6450987384774721,\n",
       " 0.6449856226001232,\n",
       " 0.6448734808776538,\n",
       " 0.6447623045816493,\n",
       " 0.6446520850647087,\n",
       " 0.6445428137596729,\n",
       " 0.6444344821788626,\n",
       " 0.6443270819133198,\n",
       " 0.6442206046320603,\n",
       " 0.6441150420813319,\n",
       " 0.6440103860838768,\n",
       " 0.6439066285382066,\n",
       " 0.6438037614178788,\n",
       " 0.6437017767707838,\n",
       " 0.6436006667184366,\n",
       " 0.6435004234552767,\n",
       " 0.6434010392479722,\n",
       " 0.6433025064347347,\n",
       " 0.6432048174246369,\n",
       " 0.6431079646969374,\n",
       " 0.6430119408004146,\n",
       " 0.642916738352703,\n",
       " 0.6428223500396393,\n",
       " 0.6427287686146124,\n",
       " 0.6426359868979213,\n",
       " 0.6425439977761371,\n",
       " 0.6424527942014737,\n",
       " 0.6423623691911626,\n",
       " 0.6422727158268344,\n",
       " 0.642183827253906,\n",
       " 0.6420956966809734,\n",
       " 0.6420083173792119,\n",
       " 0.6419216826817802,\n",
       " 0.64183578598323,\n",
       " 0.6417506207389243,\n",
       " 0.6416661804644577,\n",
       " 0.6415824587350833,\n",
       " 0.6414994491851475,\n",
       " 0.6414171455075268,\n",
       " 0.6413355414530719,\n",
       " 0.6412546308300571,\n",
       " 0.6411744075036345,\n",
       " 0.641094865395294,\n",
       " 0.6410159984823274,\n",
       " 0.6409378007972996,\n",
       " 0.640860266427523,\n",
       " 0.6407833895145381,\n",
       " 0.6407071642535987,\n",
       " 0.6406315848931619,\n",
       " 0.6405566457343835,\n",
       " 0.6404823411306186,\n",
       " 0.6404086654869254,\n",
       " 0.6403356132595758,\n",
       " 0.6402631789555688,\n",
       " 0.6401913571321515,\n",
       " 0.6401201423963404,\n",
       " 0.6400495294044518,\n",
       " 0.639979512861634,\n",
       " 0.6399100875214045,\n",
       " 0.6398412481851925,\n",
       " 0.6397729897018851,\n",
       " 0.6397053069673775,\n",
       " 0.6396381949241297,\n",
       " 0.6395716485607237,\n",
       " 0.6395056629114293,\n",
       " 0.6394402330557712,\n",
       " 0.6393753541181009,\n",
       " 0.6393110212671731,\n",
       " 0.6392472297157266,\n",
       " 0.6391839747200679,\n",
       " 0.639121251579661,\n",
       " 0.6390590556367184,\n",
       " 0.6389973822757984,\n",
       " 0.6389362269234051,\n",
       " 0.6388755850475933,\n",
       " 0.6388154521575754,\n",
       " 0.6387558238033346,\n",
       " 0.6386966955752393,\n",
       " 0.6386380631036633,\n",
       " 0.6385799220586086,\n",
       " 0.6385222681493317,\n",
       " 0.6384650971239749,\n",
       " 0.6384084047691991,\n",
       " 0.6383521869098222,\n",
       " 0.6382964394084594,\n",
       " 0.6382411581651679,\n",
       " 0.6381863391170953,\n",
       " 0.6381319782381297,\n",
       " 0.6380780715385562,\n",
       " 0.6380246150647128,\n",
       " 0.6379716048986537,\n",
       " 0.6379190371578128,\n",
       " 0.6378669079946724,\n",
       " 0.6378152135964332,\n",
       " 0.6377639501846901,\n",
       " 0.637713114015108,\n",
       " 0.6376627013771039,\n",
       " 0.6376127085935296,\n",
       " 0.6375631320203593,\n",
       " 0.6375139680463784,\n",
       " 0.6374652130928766,\n",
       " 0.6374168636133445,\n",
       " 0.6373689160931711,\n",
       " 0.6373213670493463,\n",
       " 0.6372742130301652,\n",
       " 0.6372274506149348,\n",
       " 0.6371810764136856,\n",
       " 0.6371350870668832,\n",
       " 0.6370894792451448,\n",
       " 0.6370442496489578,\n",
       " 0.6369993950084009,\n",
       " 0.6369549120828679,\n",
       " 0.6369107976607947,\n",
       " 0.6368670485593882,\n",
       " 0.6368236616243584,\n",
       " 0.6367806337296527,\n",
       " 0.636737961777193,\n",
       " 0.6366956426966158,\n",
       " 0.6366536734450132,\n",
       " 0.6366120510066786,\n",
       " 0.636570772392853,\n",
       " 0.6365298346414748,\n",
       " 0.6364892348169319,\n",
       " 0.636448970009815,\n",
       " 0.6364090373366763,\n",
       " 0.636369433939786,\n",
       " 0.6363301569868958,\n",
       " 0.6362912036710009,\n",
       " 0.6362525712101074,\n",
       " 0.6362142568469991,\n",
       " 0.6361762578490088,\n",
       " 0.6361385715077904,\n",
       " 0.6361011951390945,\n",
       " 0.6360641260825441,\n",
       " 0.6360273617014152,\n",
       " 0.6359908993824164,\n",
       " 0.6359547365354736,\n",
       " 0.6359188705935153,\n",
       " 0.6358832990122587,\n",
       " 0.6358480192700013,\n",
       " 0.6358130288674109,\n",
       " 0.6357783253273196,\n",
       " 0.6357439061945196,\n",
       " 0.6357097690355603,\n",
       " 0.6356759114385482,\n",
       " 0.6356423310129475,\n",
       " 0.6356090253893845,\n",
       " 0.6355759922194515,\n",
       " 0.6355432291755149,\n",
       " 0.635510733950523,\n",
       " 0.6354785042578182,\n",
       " 0.6354465378309477,\n",
       " 0.6354148324234792,\n",
       " 0.6353833858088163,\n",
       " 0.6353521957800164,\n",
       " 0.635321260149611,\n",
       " 0.6352905767494255,\n",
       " 0.635260143430404,\n",
       " 0.6352299580624328,\n",
       " 0.6352000185341665,\n",
       " 0.6351703227528578,\n",
       " 0.6351408686441851,\n",
       " 0.635111654152085,\n",
       " 0.6350826772385851,\n",
       " 0.6350539358836382,\n",
       " 0.6350254280849583,\n",
       " 0.6349971518578588,\n",
       " 0.6349691052350911,\n",
       " 0.6349412862666858,\n",
       " 0.6349136930197942,\n",
       " 0.6348863235785331,\n",
       " 0.6348591760438294,\n",
       " 0.634832248533266,\n",
       " 0.634805539180932,\n",
       " 0.6347790461372702,\n",
       " 0.6347527675689293,\n",
       " 0.6347267016586161,\n",
       " 0.6347008466049492,\n",
       " 0.6346752006223142,\n",
       " 0.6346497619407205,\n",
       " 0.634624528805659,\n",
       " 0.6345994994779618,\n",
       " 0.6345746722336623,\n",
       " 0.6345500453638574,\n",
       " 0.634525617174571,\n",
       " 0.6345013859866181,\n",
       " 0.6344773501354714,\n",
       " 0.6344535079711268,\n",
       " 0.6344298578579739,\n",
       " 0.634406398174664,\n",
       " 0.6343831273139816,\n",
       " 0.6343600436827161,\n",
       " 0.6343371457015357,\n",
       " 0.6343144318048609,\n",
       " 0.6342919004407412,\n",
       " 0.6342695500707309,\n",
       " 0.6342473791697679,\n",
       " 0.6342253862260524,\n",
       " 0.6342035697409272,\n",
       " 0.634181928228759,\n",
       " 0.6341604602168212,\n",
       " 0.6341391642451774,\n",
       " 0.6341180388665657,\n",
       " 0.6340970826462854,\n",
       " 0.6340762941620827,\n",
       " 0.6340556720040396,\n",
       " 0.634035214774462,\n",
       " 0.6340149210877712,\n",
       " 0.6339947895703926,\n",
       " 0.6339748188606502,\n",
       " 0.6339550076086582,\n",
       " 0.6339353544762152,\n",
       " 0.6339158581367,\n",
       " 0.6338965172749673,\n",
       " 0.6338773305872438,\n",
       " 0.6338582967810279,\n",
       " 0.6338394145749875,\n",
       " 0.6338206826988599,\n",
       " 0.6338020998933535,\n",
       " 0.633783664910048,\n",
       " 0.633765376511299,\n",
       " 0.6337472334701398,\n",
       " 0.6337292345701871,\n",
       " 0.6337113786055462,\n",
       " 0.6336936643807157,\n",
       " 0.6336760907104975,\n",
       " 0.6336586564199022,\n",
       " 0.633641360344059,\n",
       " 0.6336242013281257,\n",
       " 0.6336071782271986,\n",
       " 0.633590289906224,\n",
       " 0.6335735352399109,\n",
       " 0.633556913112643,\n",
       " 0.6335404224183937,\n",
       " 0.6335240620606395,\n",
       " 0.6335078309522766,\n",
       " 0.6334917280155363,\n",
       " 0.633475752181902,\n",
       " 0.6334599023920273,\n",
       " 0.6334441775956543,\n",
       " 0.6334285767515327,\n",
       " 0.6334130988273401,\n",
       " 0.6333977427996025,\n",
       " 0.6333825076536158,\n",
       " 0.6333673923833684,\n",
       " 0.6333523959914631,\n",
       " 0.6333375174890423,\n",
       " 0.6333227558957104,\n",
       " 0.6333081102394607,\n",
       " 0.6332935795566,\n",
       " 0.6332791628916752,\n",
       " 0.6332648592974004,\n",
       " 0.6332506678345851,\n",
       " 0.633236587572062,\n",
       " 0.6332226175866162,\n",
       " 0.6332087569629153,\n",
       " 0.6331950047934398,\n",
       " 0.6331813601784133,\n",
       " 0.6331678222257351,\n",
       " 0.6331543900509119,\n",
       " 0.6331410627769913,\n",
       " 0.6331278395344947,\n",
       " 0.6331147194613516,\n",
       " 0.6331017017028343,\n",
       " 0.6330887854114937,\n",
       " 0.6330759697470948,\n",
       " 0.6330632538765534,\n",
       " 0.6330506369738721,\n",
       " 0.6330381182200805,\n",
       " 0.6330256968031708,\n",
       " 0.6330133719180379,\n",
       " 0.6330011427664188,\n",
       " 0.6329890085568322,\n",
       " 0.6329769685045189,\n",
       " 0.6329650218313829,\n",
       " 0.6329531677659336,\n",
       " 0.6329414055432265,\n",
       " 0.632929734404807,\n",
       " 0.6329181535986534,\n",
       " 0.6329066623791196,\n",
       " 0.6328952600068806,\n",
       " 0.6328839457488763,\n",
       " 0.6328727188782567,\n",
       " 0.6328615786743284,\n",
       " 0.6328505244224994,\n",
       " 0.632839555414227,\n",
       " 0.6328286709469644,\n",
       " 0.6328178703241085,\n",
       " 0.6328071528549474,\n",
       " 0.63279651785461,\n",
       " 0.6327859646440143,\n",
       " 0.632775492549817,\n",
       " 0.6327651009043639,\n",
       " 0.6327547890456396,\n",
       " 0.6327445563172188,\n",
       " 0.6327344020682181,\n",
       " 0.6327243256532459,\n",
       " 0.6327143264323574,\n",
       " 0.6327044037710045,\n",
       " 0.6326945570399907,\n",
       " 0.6326847856154236,\n",
       " 0.6326750888786693,\n",
       " 0.6326654662163064,\n",
       " 0.6326559170200805,\n",
       " 0.63264644068686,\n",
       " 0.6326370366185912,\n",
       " 0.6326277042222543,\n",
       " 0.6326184429098195,\n",
       " 0.6326092520982044,\n",
       " 0.6326001312092299,\n",
       " 0.6325910796695793,\n",
       " 0.6325820969107547,\n",
       " 0.6325731823690363,\n",
       " 0.6325643354854403,\n",
       " 0.6325555557056783,\n",
       " 0.6325468424801169,\n",
       " 0.6325381952637366,\n",
       " 0.632529613516093,\n",
       " 0.6325210967012761,\n",
       " 0.6325126442878723,\n",
       " 0.6325042557489244,\n",
       " 0.6324959305618943,\n",
       " 0.6324876682086236,\n",
       " 0.632479468175297,\n",
       " 0.6324713299524035,\n",
       " 0.6324632530347009,\n",
       " 0.6324552369211776,\n",
       " 0.6324472811150169,\n",
       " 0.6324393851235606,\n",
       " 0.632431548458273,\n",
       " 0.6324237706347059,\n",
       " 0.6324160511724629,\n",
       " 0.632408389595165,\n",
       " 0.6324007854304152,\n",
       " 0.6323932382097659,\n",
       " 0.632385747468683,\n",
       " 0.6323783127465136,\n",
       " 0.6323709335864518,\n",
       " 0.6323636095355066,\n",
       " 0.6323563401444682,\n",
       " 0.6323491249678764,\n",
       " 0.6323419635639874,\n",
       " 0.632334855494743,\n",
       " 0.6323278003257384,\n",
       " 0.632320797626191,\n",
       " 0.6323138469689087,\n",
       " 0.6323069479302609,\n",
       " 0.6323001000901458,\n",
       " 0.6322933030319622,\n",
       " 0.6322865563425778,\n",
       " 0.6322798596123007,\n",
       " 0.6322732124348497,\n",
       " 0.6322666144073242,\n",
       " 0.6322600651301772,\n",
       " 0.6322535642071848,\n",
       " 0.6322471112454189,\n",
       " 0.6322407058552182,\n",
       " 0.6322343476501612,\n",
       " 0.6322280362470384,\n",
       " 0.6322217712658239,\n",
       " 0.6322155523296494,\n",
       " 0.6322093790647768,\n",
       " 0.6322032511005715,\n",
       " 0.6321971680694759,\n",
       " 0.632191129606983,\n",
       " 0.6321851353516115,\n",
       " 0.6321791849448783,\n",
       " 0.6321732780312745,\n",
       " 0.632167414258239,\n",
       " 0.632161593276134,\n",
       " 0.6321558147382202,\n",
       " 0.6321500783006319,\n",
       " 0.6321443836223524,\n",
       " 0.6321387303651902,\n",
       " 0.6321331181937546,\n",
       " 0.6321275467754325,\n",
       " 0.6321220157803639,\n",
       " 0.6321165248814193,\n",
       " 0.6321110737541765,\n",
       " 0.6321056620768971,\n",
       " 0.6321002895305038,\n",
       " 0.632094955798559,\n",
       " 0.6320896605672401,\n",
       " 0.6320844035253201,\n",
       " 0.632079184364143,\n",
       " 0.6320740027776044,\n",
       " 0.6320688584621279,\n",
       " 0.6320637511166447,\n",
       " 0.6320586804425726,\n",
       " 0.6320536461437942,\n",
       " 0.6320486479266364,\n",
       " 0.6320436854998499,\n",
       " 0.6320387585745887,\n",
       " 0.6320338668643893,\n",
       " 0.6320290100851512,\n",
       " 0.6320241879551165,\n",
       " 0.6320194001948504,\n",
       " 0.6320146465272211,\n",
       " 0.6320099266773812,\n",
       " 0.6320052403727476,\n",
       " 0.6320005873429828,\n",
       " 0.6319959673199761,\n",
       " 0.6319913800378242,\n",
       " 0.6319868252328134,\n",
       " 0.6319823026434005,\n",
       " 0.6319778120101952,\n",
       " 0.6319733530759405,\n",
       " 0.6319689255854971,\n",
       " 0.6319645292858235,\n",
       " 0.6319601639259588,\n",
       " 0.6319558292570062,\n",
       " 0.6319515250321147,\n",
       " 0.6319472510064618,\n",
       " 0.631943006937237,\n",
       " 0.6319387925836246,\n",
       " 0.6319346077067869,\n",
       " 0.6319304520698477,\n",
       " 0.6319263254378759,\n",
       " 0.6319222275778688,\n",
       " 0.6319181582587364,\n",
       " 0.6319141172512853,\n",
       " 0.631910104328202,\n",
       " 0.6319061192640382,\n",
       " 0.6319021618351944,\n",
       " 0.631898231819905,\n",
       " 0.6318943289982222,\n",
       " 0.6318904531520017,\n",
       " 0.6318866040648865,\n",
       " 0.6318827815222928,\n",
       " 0.6318789853113951,\n",
       " 0.6318752152211106,\n",
       " 0.6318714710420856,\n",
       " 0.6318677525666807,\n",
       " 0.6318640595889566,\n",
       " 0.631860391904659,\n",
       " 0.6318567493112061,\n",
       " 0.6318531316076733,\n",
       " 0.6318495385947795,\n",
       " 0.6318459700748743,\n",
       " 0.6318424258519232,\n",
       " 0.6318389057314951,\n",
       " 0.6318354095207478,\n",
       " 0.6318319370284161,\n",
       " 0.6318284880647974,\n",
       " 0.6318250624417392,\n",
       " 0.6318216599726268,\n",
       " 0.6318182804723691,\n",
       " 0.6318149237573867,\n",
       " 0.6318115896455994,\n",
       " 0.6318082779564138,\n",
       " 0.6318049885107101,\n",
       " 0.6318017211308307,\n",
       " 0.6317984756405675,\n",
       " 0.6317952518651502,\n",
       " 0.6317920496312339,\n",
       " 0.6317888687668878,\n",
       " 0.6317857091015828,\n",
       " 0.6317825704661802,\n",
       " 0.6317794526929206,\n",
       " 0.6317763556154111,\n",
       " 0.6317732790686152,\n",
       " 0.6317702228888412,\n",
       " 0.6317671869137307,\n",
       " 0.6317641709822476,\n",
       " 0.6317611749346677,\n",
       " 0.6317581986125668,\n",
       " 0.631755241858811,\n",
       " 0.631752304517545,\n",
       " 0.6317493864341825,\n",
       " 0.6317464874553944,\n",
       " 0.6317436074290995,\n",
       " 0.6317407462044539,\n",
       " 0.63173790363184,\n",
       " 0.6317350795628575,\n",
       " 0.6317322738503124,\n",
       " 0.6317294863482075,\n",
       " 0.6317267169117321,\n",
       " 0.6317239653972525,\n",
       " 0.6317212316623023,\n",
       " 0.6317185155655722,\n",
       " 0.6317158169669013,\n",
       " 0.6317131357272665,\n",
       " 0.6317104717087738,\n",
       " 0.6317078247746495,\n",
       " 0.6317051947892295,\n",
       " 0.6317025816179512,\n",
       " 0.6316999851273439,\n",
       " 0.6316974051850204,\n",
       " 0.6316948416596674,\n",
       " 0.6316922944210365,\n",
       " 0.6316897633399359,\n",
       " 0.6316872482882219,\n",
       " 0.6316847491387897,\n",
       " 0.6316822657655645,\n",
       " 0.6316797980434942,\n",
       " 0.6316773458485396,\n",
       " 0.6316749090576672,\n",
       " 0.6316724875488403,\n",
       " 0.6316700812010105,\n",
       " 0.6316676898941103,\n",
       " 0.6316653135090445,\n",
       " 0.6316629519276823,\n",
       " 0.6316606050328494,\n",
       " 0.6316582727083202,\n",
       " 0.6316559548388099,\n",
       " 0.6316536513099662,\n",
       " 0.6316513620083631,\n",
       " 0.6316490868214915,\n",
       " 0.6316468256377528,\n",
       " 0.631644578346451,\n",
       " 0.6316423448377857,\n",
       " 0.631640125002844,\n",
       " 0.6316379187335938,\n",
       " 0.631635725922876,\n",
       " 0.6316335464643984,\n",
       " 0.6316313802527271,\n",
       " 0.6316292271832805,\n",
       " 0.6316270871523223,\n",
       " 0.6316249600569536,\n",
       " 0.6316228457951073,\n",
       " 0.6316207442655402,\n",
       " 0.6316186553678267,\n",
       " 0.6316165790023525,\n",
       " 0.6316145150703071,\n",
       " 0.6316124634736778,\n",
       " 0.6316104241152426,\n",
       " 0.6316083968985646,\n",
       " 0.6316063817279847,\n",
       " 0.6316043785086155,\n",
       " 0.6316023871463355,\n",
       " 0.6316004075477819,\n",
       " 0.6315984396203449,\n",
       " 0.6315964832721618,\n",
       " 0.6315945384121098,\n",
       " 0.6315926049498016,\n",
       " 0.6315906827955778,\n",
       " 0.631588771860502,\n",
       " 0.6315868720563543,\n",
       " 0.6315849832956255,\n",
       " 0.6315831054915115,\n",
       " 0.6315812385579076,\n",
       " 0.6315793824094025,\n",
       " 0.6315775369612724,\n",
       " 0.6315757021294764,\n",
       " 0.6315738778306497,\n",
       " 0.6315720639820985,\n",
       " 0.6315702605017952,\n",
       " 0.6315684673083719,\n",
       " 0.6315666843211157,\n",
       " 0.631564911459963,\n",
       " 0.6315631486454943,\n",
       " 0.6315613957989291,\n",
       " 0.6315596528421206,\n",
       " 0.63155791969755,\n",
       " 0.6315561962883227,\n",
       " 0.6315544825381615,\n",
       " 0.631552778371403,\n",
       " 0.6315510837129914,\n",
       " 0.631549398488475,\n",
       " 0.6315477226239997,\n",
       " 0.6315460560463052,\n",
       " 0.6315443986827195,\n",
       " 0.6315427504611549,\n",
       " 0.6315411113101023,\n",
       " 0.6315394811586271,\n",
       " 0.6315378599363644,\n",
       " 0.6315362475735141,\n",
       " 0.6315346440008368,\n",
       " 0.6315330491496488,\n",
       " 0.6315314629518177,\n",
       " 0.6315298853397578,\n",
       " 0.6315283162464264,\n",
       " 0.6315267556053176,\n",
       " 0.6315252033504605,\n",
       " 0.631523659416412,\n",
       " 0.6315221237382551,\n",
       " 0.6315205962515926,\n",
       " 0.6315190768925446,\n",
       " 0.6315175655977429,\n",
       " 0.631516062304327,\n",
       " 0.6315145669499413,\n",
       " 0.6315130794727292,\n",
       " 0.6315115998113301,\n",
       " 0.6315101279048756,\n",
       " 0.6315086636929843,\n",
       " 0.6315072071157593,\n",
       " 0.631505758113783,\n",
       " 0.6315043166281139,\n",
       " 0.6315028826002831,\n",
       " 0.6315014559722892,\n",
       " 0.6315000366865958,\n",
       " 0.6314986246861272,\n",
       " 0.631497219914264,\n",
       " 0.631495822314841,\n",
       " 0.631494431832142,\n",
       " 0.6314930484108967,\n",
       " 0.6314916719962772,\n",
       " 0.6314903025338946,\n",
       " 0.6314889399697947,\n",
       " 0.6314875842504554,\n",
       " 0.6314862353227824,\n",
       " 0.631484893134106,\n",
       " 0.6314835576321782,\n",
       " 0.6314822287651684,\n",
       " 0.6314809064816607,\n",
       " 0.6314795907306496,\n",
       " 0.6314782814615385,\n",
       " 0.6314769786241342,\n",
       " 0.6314756821686452,\n",
       " 0.6314743920456781,\n",
       " 0.6314731082062335,\n",
       " 0.6314718306017039,\n",
       " 0.6314705591838704,\n",
       " 0.6314692939048989,\n",
       " 0.6314680347173375,\n",
       " 0.6314667815741133,\n",
       " 0.6314655344285292,\n",
       " 0.6314642932342615,\n",
       " 0.6314630579453554,\n",
       " 0.6314618285162242,\n",
       " 0.6314606049016439,\n",
       " 0.6314593870567525,\n",
       " 0.6314581749370451,\n",
       " 0.631456968498373,\n",
       " 0.6314557676969388,\n",
       " 0.631454572489295,\n",
       " 0.631453382832341,\n",
       " 0.6314521986833196,\n",
       " 0.6314510199998147,\n",
       " 0.6314498467397486,\n",
       " 0.6314486788613791,\n",
       " 0.6314475163232971,\n",
       " 0.6314463590844233,\n",
       " 0.6314452071040061,\n",
       " 0.6314440603416186,\n",
       " 0.6314429187571564,\n",
       " 0.6314417823108345,\n",
       " 0.6314406509631851,\n",
       " 0.631439524675055,\n",
       " 0.6314384034076028,\n",
       " 0.6314372871222964,\n",
       " 0.631436175780911,\n",
       " 0.6314350693455262,\n",
       " 0.6314339677785233,\n",
       " 0.6314328710425838,\n",
       " 0.6314317791006859,\n",
       " 0.6314306919161026,\n",
       " 0.6314296094523998,\n",
       " 0.6314285316734327,\n",
       " 0.6314274585433449,\n",
       " 0.6314263900265652,\n",
       " 0.6314253260878051,\n",
       " 0.6314242666920574,\n",
       " 0.6314232118045937,\n",
       " 0.6314221613909612,\n",
       " 0.6314211154169815,\n",
       " 0.6314200738487484,\n",
       " 0.6314190366526252,\n",
       " 0.6314180037952425,\n",
       " 0.6314169752434964,\n",
       " 0.6314159509645468,\n",
       " 0.6314149309258138,\n",
       " 0.6314139150949775,\n",
       " 0.6314129034399739,\n",
       " 0.6314118959289948,\n",
       " 0.6314108925304845,\n",
       " 0.631409893213138,\n",
       " 0.631408897945899,\n",
       " 0.6314079066979587,\n",
       " 0.6314069194387519,\n",
       " 0.6314059361379575,\n",
       " 0.6314049567654945,\n",
       " 0.6314039812915209,\n",
       " 0.631403009686432,\n",
       " 0.6314020419208581,\n",
       " 0.631401077965663,\n",
       " 0.6314001177919413,\n",
       " 0.6313991613710175,\n",
       " 0.631398208674444,\n",
       " 0.6313972596739985,\n",
       " 0.6313963143416831,\n",
       " 0.6313953726497219,\n",
       " 0.6313944345705598,\n",
       " 0.63139350007686,\n",
       " 0.6313925691415028,\n",
       " 0.6313916417375838,\n",
       " 0.6313907178384118,\n",
       " 0.6313897974175073,\n",
       " 0.6313888804486016,\n",
       " 0.6313879669056333,\n",
       " 0.6313870567627484,\n",
       " 0.6313861499942977,\n",
       " 0.6313852465748353,\n",
       " 0.6313843464791176,\n",
       " 0.6313834496821003,\n",
       " 0.6313825561589382,\n",
       " 0.6313816658849833,\n",
       " 0.6313807788357823,\n",
       " 0.6313798949870759,\n",
       " 0.6313790143147976,\n",
       " 0.6313781367950712,\n",
       " 0.6313772624042093,\n",
       " 0.6313763911187129,\n",
       " 0.6313755229152689,\n",
       " 0.6313746577707486,\n",
       " 0.631373795662207,\n",
       " 0.6313729365668808,\n",
       " 0.6313720804621862,\n",
       " 0.6313712273257192,\n",
       " 0.6313703771352528,\n",
       " 0.6313695298687358,\n",
       " 0.6313686855042924,\n",
       " 0.6313678440202188,\n",
       " 0.631367005394984,\n",
       " 0.6313661696072268,\n",
       " 0.6313653366357553,\n",
       " 0.6313645064595454,\n",
       " 0.6313636790577395,\n",
       " 0.6313628544096445,\n",
       " 0.6313620324947312,\n",
       " 0.6313612132926337,\n",
       " 0.6313603967831457,\n",
       " 0.6313595829462219,\n",
       " 0.6313587717619751,\n",
       " 0.6313579632106756,\n",
       " 0.6313571572727494,\n",
       " 0.6313563539287774,\n",
       " 0.6313555531594944,\n",
       " 0.6313547549457867,\n",
       " 0.631353959268693,\n",
       " 0.6313531661094004,\n",
       " 0.6313523754492459,\n",
       " 0.6313515872697133,\n",
       " 0.6313508015524331,\n",
       " 0.6313500182791807,\n",
       " 0.6313492374318758,\n",
       " 0.6313484589925806,\n",
       " 0.6313476829434995,\n",
       " 0.6313469092669769,\n",
       " 0.6313461379454972,\n",
       " 0.6313453689616828,\n",
       " 0.6313446022982937,\n",
       " 0.6313438379382256,\n",
       " 0.6313430758645093,\n",
       " 0.6313423160603102,\n",
       " 0.631341558508926,\n",
       " 0.6313408031937867,\n",
       " 0.6313400500984524,\n",
       " 0.6313392992066139,\n",
       " 0.63133855050209,\n",
       " 0.6313378039688275,\n",
       " 0.6313370595909,\n",
       " 0.6313363173525067,\n",
       " 0.6313355772379714,\n",
       " 0.6313348392317414,\n",
       " 0.631334103318387,\n",
       " 0.6313333694826003,\n",
       " 0.6313326377091939,\n",
       " 0.6313319079831001,\n",
       " 0.6313311802893703,\n",
       " 0.6313304546131738,\n",
       " 0.6313297309397966,\n",
       " 0.631329009254641,\n",
       " 0.6313282895432241,\n",
       " 0.6313275717911773,\n",
       " 0.6313268559842455,\n",
       " 0.6313261421082855,\n",
       " 0.631325430149266,\n",
       " 0.6313247200932662,\n",
       " 0.6313240119264747,\n",
       " 0.6313233056351892,\n",
       " 0.6313226012058154,\n",
       " 0.6313218986248659,\n",
       " 0.6313211978789599,\n",
       " 0.6313204989548215,\n",
       " 0.6313198018392798,\n",
       " 0.6313191065192676,\n",
       " 0.6313184129818203,\n",
       " 0.6313177212140755,\n",
       " 0.6313170312032725,\n",
       " 0.6313163429367507,\n",
       " 0.6313156564019491,\n",
       " 0.6313149715864056,\n",
       " 0.6313142884777565,\n",
       " 0.6313136070637352,\n",
       " 0.6313129273321717,\n",
       " 0.6313122492709915,\n",
       " 0.6313115728682157,\n",
       " 0.6313108981119594,\n",
       " 0.6313102249904307,\n",
       " 0.6313095534919313,\n",
       " 0.6313088836048544,\n",
       " 0.6313082153176847,\n",
       " 0.6313075486189972,\n",
       " 0.6313068834974571,\n",
       " 0.6313062199418189,\n",
       " 0.6313055579409248,\n",
       " 0.6313048974837056,\n",
       " 0.6313042385591782,\n",
       " 0.6313035811564467,\n",
       " 0.6313029252647006,\n",
       " 0.631302270873214,\n",
       " 0.6313016179713459,\n",
       " 0.6313009665485384,\n",
       " 0.6313003165943167,\n",
       " 0.6312996680982887,\n",
       " 0.6312990210501436,\n",
       " 0.6312983754396515,\n",
       " 0.6312977312566631,\n",
       " 0.6312970884911088,\n",
       " 0.6312964471329982,\n",
       " 0.631295807172419,\n",
       " 0.6312951685995369,\n",
       " 0.6312945314045951,\n",
       " 0.6312938955779133,\n",
       " 0.631293261109887,\n",
       " 0.6312926279909871,\n",
       " 0.6312919962117596,\n",
       " 0.6312913657628245,\n",
       " 0.6312907366348756,\n",
       " 0.6312901088186794,\n",
       " 0.6312894823050751,\n",
       " 0.6312888570849737,\n",
       " 0.631288233149358,\n",
       " 0.6312876104892806,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ba50f28>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation de votre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons évaluer la performance du modèle de deux façons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluez la probabilité qu'un étudiant ayant obtenu 45 au premier examen, et 85 au deuxième, soit admis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devriez avoir une probabilité d'admission de 0.776"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluer l'exactitude (accuracy) des prédictions faites sur les données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez votre fonction _predict_ sur les données d'entraînement (X) et récupérez les prédictions dans un vecteur p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59725911]\n",
      " [0.58159797]\n",
      " [0.59966471]\n",
      " [0.66096799]\n",
      " [0.7026212 ]\n",
      " [0.62056974]\n",
      " [0.6644169 ]\n",
      " [0.69030333]\n",
      " [0.69755796]\n",
      " [0.71071022]\n",
      " [0.73421517]\n",
      " [0.68836777]\n",
      " [0.70989999]\n",
      " [0.68365103]\n",
      " [0.60936787]\n",
      " [0.64654282]\n",
      " [0.67750872]\n",
      " [0.6741767 ]\n",
      " [0.6860178 ]\n",
      " [0.69480258]\n",
      " [0.67237379]\n",
      " [0.72441421]\n",
      " [0.63301025]\n",
      " [0.59165551]\n",
      " [0.69942947]\n",
      " [0.66383623]\n",
      " [0.70159763]\n",
      " [0.72860025]\n",
      " [0.66033363]\n",
      " [0.60600797]\n",
      " [0.66209644]\n",
      " [0.7143531 ]\n",
      " [0.63869111]\n",
      " [0.63935274]\n",
      " [0.6104686 ]\n",
      " [0.64340477]\n",
      " [0.59827089]\n",
      " [0.66965331]\n",
      " [0.68917695]\n",
      " [0.59576963]\n",
      " [0.71103172]\n",
      " [0.63522347]\n",
      " [0.73428783]\n",
      " [0.70588639]\n",
      " [0.63386953]\n",
      " [0.6614853 ]\n",
      " [0.69798517]\n",
      " [0.74332836]\n",
      " [0.66671222]\n",
      " [0.73088905]\n",
      " [0.70448491]\n",
      " [0.74359564]\n",
      " [0.72373879]\n",
      " [0.59463371]\n",
      " [0.6325265 ]\n",
      " [0.63211558]\n",
      " [0.74116297]\n",
      " [0.59444101]\n",
      " [0.69133932]\n",
      " [0.68684564]\n",
      " [0.69579825]\n",
      " [0.5947619 ]\n",
      " [0.64564704]\n",
      " [0.58179002]\n",
      " [0.62087612]\n",
      " [0.67027518]\n",
      " [0.61451994]\n",
      " [0.62982383]\n",
      " [0.70730082]\n",
      " [0.67316064]\n",
      " [0.58774224]\n",
      " [0.66897663]\n",
      " [0.6902187 ]\n",
      " [0.65996016]\n",
      " [0.65648064]\n",
      " [0.74591527]\n",
      " [0.63015973]\n",
      " [0.6363189 ]\n",
      " [0.65610499]\n",
      " [0.70582005]\n",
      " [0.72325742]\n",
      " [0.73291964]\n",
      " [0.67517169]\n",
      " [0.6505937 ]\n",
      " [0.70735687]\n",
      " [0.68013498]\n",
      " [0.61607485]\n",
      " [0.6965328 ]\n",
      " [0.70423484]\n",
      " [0.63896947]\n",
      " [0.73483577]\n",
      " [0.728441  ]\n",
      " [0.64331378]\n",
      " [0.69367498]\n",
      " [0.72248259]\n",
      " [0.70922398]\n",
      " [0.61762978]\n",
      " [0.74450907]\n",
      " [0.64673153]\n",
      " [0.69486363]]\n"
     ]
    }
   ],
   "source": [
    "p = predict(X, theta)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez le pourcentage des éléments de p qui correspondent à ceux de y. Ça vous donne le score d'exactitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "accuracy = y - p\n",
    "score = np.where(accuracy == 0)[1].size\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devriez avoir un score d'environ 89.0 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelle est la précision, le recall et le F1-score de votre modele ? (écrivez trois fonctions pour obtenir chacunes de ces métriques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A l'aide de l'hyperparameter tuning (random search), trouvez les alpha et lambda qui permettent de maximiser le F1-score. Vous devrez entrainer plusieurs fois votre modele à l'aide de la fonction fit pour trouver ces parametres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Visualisez la frontière de décision (decision boundary) sur le graphe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ceux qui veulent découvrir Matplotlib, il faut ici afficher les données en deux nuages de points distincts (pour les deux classes) sur le même graphe, et aussi trouver une façon de tracer la fonction qui définit la frontière de décision. Amusez-vous bien, et surtout aidez-vous! Voici un exemple de ce que ça devrait donner:  \n",
    "<img src=\"figure-2.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
